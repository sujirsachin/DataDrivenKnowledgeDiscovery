---
title: "Lab 02 Linear Regression by- Sachin Mohan Sujir"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
#  PART I

# Loading the data and viewing an abstract summary of the dataset
```{r}
Auto=read.csv("Auto.csv",header=T, na.strings="?")
Auto = na.omit(Auto)
summary(Auto)
```

# (a) Scatterplot matrix that includes all variables in the dataset

```{r}
pairs(Auto)
```


# (b) Matrix of correlation between the variables excluding 'names' as it is Qualitative.

I am using select to exclude the names by using '-name' which excludes the term.

```{r}
cor(subset(Auto,select = -name))
```


# (c) Multiple linear regression using all variables as predictors and mpg as the response, also excluding name variable.

```{r}
linearModel=lm(mpg~.-name,data=Auto)
summary(linearModel)
```


# Inferences

(i) Yes, there exists a relationship between the predictors and the response. As seen in the summary the f-statistic is 252.4 which is way ahead of the p-value which is very small. Hence we can conclude that there exists a relationship as we use f-statistc to determine if there exists a relationship for multiple regression.

(ii) The predictors- weight, year, origin and displacement seem to have statistical significant relationship to the response as these variable reject the null hypothesis and accept the alternate hypothesis. I determined this using the associated p-value and its t-statistic and similarly cylinders, horsepower,accelaration do not have statistical significant relationship.

(iii) The coefficient of year is 0.750773. This means that the mpg increases by 0.750773 for every one year of model. So approximately the fuel efficiency gets better by almost 1 mpg/one year.

From the summary, looking at the Residual standard error we can infer that predicted mpg might be 3.328 units more than the actual value or 3.328 less than the actual value based on the RSE.
Also, the multiple R-squared is 0.8215 which is close to one. So the model can be a very good fit.


# (d) Diagnostic plot to determine problems with fit, outliers and high leverage points.

```{r}
par(mfrow=c(2,2))
plot(linearModel)
```



# Inferences

(i) From the residuals vs fitted values, it can be seen that there exists some non-linear relationship that has not been captured. There are residual points outside the fitted curve. It means we can have a non-linear fit.

(ii) We can also see from the normal Q-Q that there are outliers. Usually if residual errors are in a normal distribution, we will have a straight line. But here we see data outside the stright line- 323, 326, 327 are outside the straight lines and are outliers.

(iii) From the leverage plot, the presence of aoutliers (higher than 2 or lower than -2) and one high leverage point- point 14.

To get a clearer picture of outliers we can plot prediction vs residuals

```{r}
plot(predict(linearModel),residuals(linearModel))
```



In this plot we get a clear picture of outliers. The values outside 10 are the outliers.

# (e) Interaction effect

(i) Considering interaction effect between cylinders and displacement
```{r}
attach(Auto)

model1=lm(mpg~cylinders*displacement,data=Auto)
summary(model1)
```
The interaction effect between cylinders and displacement is significant.

(ii) Consider interaction effect between displacement and cylinders and displacement and weight(highest correlation from the correlation matrix)

```{r}

model2=lm(mpg~cylinders*displacement+displacement*weight,data=Auto)
summary(model2)
```

From the p-values, we can see that the interaction between displacement and weight is statistically signifcant, while the interaction between cylinders and displacement is not significant.

(iii) Consider interaction effect between weight and acceleration along with origin and year

```{r}
model3=lm(mpg~. - name + weight:acceleration,data=Auto)
summary(model3)
```

Here we say that there is a significant interaction effect and also R-squared has increased compared to the first linear model that we used which includes all variables except for name. Also the RSE has decreased.

(iv) Finally lets consider interaction effect between year:origin,displacement:weight,origin:displacement after removing cylinders,horsepower, acceleration and name.
```{r}
attach(Auto)
Auto1=Auto[,c(-2,-6,-4,-9)]
model4=lm(mpg~year*origin+displacement*weight+origin:displacement,data = Auto1)
summary(model4)
```
We see that the r-squared for this is 0.8576 which is a very good fit. We also see that the RSE has significantly decreased. The interaction effect between origin and displacement isn't significant.

# (f) Trying transformation of variables.

```{r}
Tmodel1=lm(mpg~log(horsepower),data=Auto)
summary(Tmodel1)
```
```{r}
Tmodel2=lm(mpg~sqrt(horsepower),data=Auto)
summary(Tmodel2)
```

```{r}
Tmodel3=lm(mpg~I(horsepower^2),data=Auto)
summary(Tmodel3)
```
Transforming horsepower to Log(horsepower) has the lowest RSE as compared to sqrt(horsepower), I(horsepower^2).

```{r}
Tmodel4=lm(mpg~log(year)+I(displacement^2)+sqrt(origin),data = Auto1)
summary(Tmodel4)
```

Finally, after overall analysis it is seen that the model 1 gives us a good fit for this data. We had Residual standard error: 2.973 and Multiple R-squared:  0.8576 which is a very good fit. So based on RSE, we can conclude given the model the predicted mpg may be greater than 2.973 or less than 2.973. And, 85.76% of the variability is explained by the model.


# Part II

For this part we use carseats dataset which is in ISLR library. We have install ISLR before we can access the library.

```{r}
library(ISLR)
fix(Carseats)
names(Carseats)
```

# (a) Fitting a regression model to predict sales using Price, Urban, US.

```{r}
attach(Carseats)
newModel=lm(Sales~Price+Urban+US)
summary(newModel)
```

# (b) Interpretation

-The Price predictor is stastically significant and has relationship towards the response. The sales is reduces by 54.459 for increase in 1 unit of price.

- The Urban predictor is a two level class with values Yes or No. Here No class takes the baseline and the UrbanYes is the dummy variable for Yes class. UrbanYes has a huge p-value and is not significant and the relationship is not satisfied. With the Urban setting, sales sales decreases by 21.916 units compared to the sales obtained by 'No' class setting.

The predictor US is an qualitative variable with 2 levels “Yes” and “No”.Here 'No' class takes the baseline and the UrbanYes is the dummy variable for 'Yes' class. USYes has a low p-value and is statistically significant and the relationship is satisfied. If the store is in US setting the sales are 1200.573 more as compared to compared to sales in stores not in US setting.

# (c) Model Equation

Sales = 13.043469 - 0.054459 * Price - 0.021916 * UrbanYes + 1.200573 * USYes +ε

# (d) Predictors that reject null hypothesis
In order to reject null hypothesis a predictor must have a low p-value, lower than the t-statistic. So with this assumption, we can reject the null hypothesis for Price and USYes and we cannot reject it for UrbanYes because the p-value is 0.936 which is high.

# (e) Model for predictors that are significant.

```{r}
newModel1=lm(Sales~Price+US)
summary(newModel1)
```


# (f)Choosing the best model

Since we have the r-squared value of the larger model and the r-squared of the smaller model equal, we check the RSE which is also more or less similar with a minute difference. Hence we choose the smaller model as model with lesser number of predictors is chosen when the two models fit the data similarly. Moreover, RSE for the smaller model is slightly less than the larger model. Hence we choose the second model.

Sales=13.03079-0.05448 * Price +1.19964 * USYes+ ε

# (g) Confidence Interval

```{r}
confint(newModel1)
```

